{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3章 ニューラルネットワーク\n",
    "\n",
    "## 3.4 3層ニューラルネットワークの実装\n",
    "\n",
    "図3-15\n",
    "\n",
    "上図に示すような3層ニューラルネットワークを対象として、その入力から出力への処理（フォワード方向への処理）を実装する。\n",
    "\n",
    "### 3.4.1 記号の確認\n",
    "\n",
    "図3-16\n",
    "\n",
    "$w_{ij}^{(k)}$: 重み<br>\n",
    "$i$: 次層の$i$番目のニューロン<br>\n",
    "$j$: 前層の$j$番目のニューロン<br>\n",
    "$k$: 第$k$層目\n",
    "\n",
    "バイアスは含まず、$i, j, k$は1から始まる。\n",
    "\n",
    "### 3.4.2 各層における信号伝達の実装\n",
    "\n",
    "入力層から第1層目の1番目のニューロンへの信号の伝達を下の図で表す。\n",
    "\n",
    "図3-17\n",
    "\n",
    "$b_i^{(k)}$: バイアス\n",
    "\n",
    "$a_1^{(1)}$を数式で表すと、\n",
    "\n",
    "$$\n",
    "a_1^{(1)} = w_{11}^{(1)} x_1 + w_{12}^{(1)} x_2 + b_1^{(1)}\n",
    "$$\n",
    "\n",
    "行列の積を用いると、第1層目の「重み付き和」は次の式でまとめて表せる\n",
    "\n",
    "$$\n",
    "{\\bf A}^{(1)} = {\\bf X W}^{(1)} + {\\bf B}^{(1)}\n",
    "$$\n",
    "$$\n",
    "{\\bf A}^{(1)} = \\begin{pmatrix}\n",
    "a_{1}^{(1)} & a_{2}^{(1)} & a_{3}^{(1)} \\\\\n",
    "\\end{pmatrix},\n",
    "{\\bf X}^{(1)} = \\begin{pmatrix}\n",
    "x_{1} & x_{2} \\\\\n",
    "\\end{pmatrix},\n",
    "{\\bf B}^{(1)} = \\begin{pmatrix}\n",
    "b_{1}^{(1)} & b_{2}^{(1)} & b_{3}^{(1)} \\\\\n",
    "\\end{pmatrix},\\\\\n",
    "{\\bf W}^{(1)} = \\begin{pmatrix}\n",
    "w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\\\\n",
    "w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "NumPyの多次元配列を使って上の式を実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape = (2, 3)\n",
      "X.shape = (2,)\n",
      "B1.shape = (3,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([1.0, 0.5])\n",
    "W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "B1 = np.array([0.1, 0.2, 0.3])\n",
    "\n",
    "print(\"W1.shape =\", W1.shape)\n",
    "print(\"X.shape =\", X.shape)\n",
    "print(\"B1.shape =\", B1.shape)\n",
    "\n",
    "A1 = np.dot(X, W1) + B1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "続いて、第1層目の活性化関数によるプロセスを見る。この活性化関数によるプロセスは下の図のように表せる。\n",
    "\n",
    "図3-18\n",
    "\n",
    "$a$: 隠れ層での重み付き和（重み付き信号とバイアスの総和）<br>\n",
    "$z$: 活性化関数で変換された信号\n",
    "$h$: 活性化関数\n",
    "\n",
    "活性化関数にシグモイド関数を使い、Pythonで実装すると、次のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 = [ 0.3  0.7  1.1]\n",
      "Z1 = [ 0.57444252  0.66818777  0.75026011]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "Z1 = sigmoid(A1)\n",
    "\n",
    "print(\"A1 =\", A1)\n",
    "print(\"Z1 =\", Z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図3-19\n",
    "\n",
    "続いて第1層から第2層目までの実装を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1.shape = (3,)\n",
      "W2.shape = (3, 2)\n",
      "B2.shape = (2,)\n"
     ]
    }
   ],
   "source": [
    "W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "B2 = np.array([0.1, 0.2])\n",
    "\n",
    "print(\"Z1.shape =\", Z1.shape)\n",
    "print(\"W2.shape =\", W2.shape)\n",
    "print(\"B2.shape =\", B2.shape)\n",
    "\n",
    "A2 = np.dot(Z1, W2) + B2\n",
    "Z2 = sigmoid(A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これは先程の入力層から第1層目までの実装と全く同じものになっている。\n",
    "\n",
    "図3-20\n",
    "\n",
    "$\\sigma$: $h$とは異なる活性化関数\n",
    "\n",
    "最後に、第2層から出力層への信号の伝達を実装する。この実装も、これまでとほとんど同じだが、最後の活性化関数だけが、これまでの隠れ層とは異なる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "W3 = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "B3 = np.array([0.1, 0.2])\n",
    "\n",
    "A3 = np.dot(Z2, W3) + B3\n",
    "Y = identity_function(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここではidentity_function()という関数を定義して出力層の活性化関数として利用した。\n",
    "このような、入力をそのまま出力する関数を**恒等関数**という。\n",
    "\n",
    "```\n",
    "出力層で利用する活性化関数は、解く問題の性質に応じて決める。\n",
    "\n",
    "回帰問題…恒等関数\n",
    "2クラス分類問題…シグモイド関数\n",
    "多クラス分類問題…ソフトマックス関数\n",
    "\n",
    "を使うのが一般的。詳しくは3.5節で扱う。\n",
    "```\n",
    "\n",
    "### 3.4.3 実装のまとめ\n",
    "\n",
    "今回行った実装をまとめて記載する。ニューラルネットワークの実装の慣例として、重みだけをW1のように大文字で表記し、それ以外は小文字で表記する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [ 0.31682708  0.69627909]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "    \n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = identity_function(a3)\n",
    "\n",
    "    return y\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "print(\"y =\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 出力層の設計\n",
    "\n",
    "ニューラルネットワークは分類問題にも回帰問題にも使える。\n",
    "\n",
    "出力層の活性化関数を変更して対応する。\n",
    "\n",
    "ソフトマックス関数は以下のように表す。\n",
    "\n",
    "$$\n",
    "y_k = \\frac{\\exp(a_k)}{\\sum_{i=1}^{n} \\exp(a_i)}\n",
    "$$\n",
    "\n",
    "恒等関数と異なり、出力の各ニューロンが全ての入力から影響を受けている。\n",
    "\n",
    "Pythonでの実装は次のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ソフトマックス関数の実装では指数関数の計算を行うことになり、オーバーフローが起こりうる。\n",
    "例えば次のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a13379/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/Users/a13379/.pyenv/versions/anaconda3-5.0.0/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ nan,  nan,  nan])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1010, 1000, 990])\n",
    "softmax(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように結果が`nan`になってしまう。\n",
    "\n",
    "そこで次のような式変形を行い、改善する。\n",
    "\n",
    "$$\n",
    "y_k = \\frac{\\exp(a_k)}{\\sum_{i=1}^{n} \\exp(a_i)}\n",
    "= \\frac{C \\exp(a_k)}{C \\sum_{i=1}^{n} \\exp(a_i)}\n",
    "= \\frac{\\exp(a_k + \\log C)}{\\sum_{i=1}^{n} \\exp(a_i + \\log C)}\n",
    "= \\frac{\\exp(a_k + C')}{\\sum_{i=1}^{n} \\exp(a_i + C')}\n",
    "$$\n",
    "\n",
    "$C'$にはどんな値を用いることも出来るが、入力信号の最大値を用いる。改善した実装は以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "すると次のように正しく計算できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.99954600e-01,   4.53978686e-05,   2.06106005e-09])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1010, 1000, 990])\n",
    "softmax(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ソフトマックス関数の特徴\n",
    "\n",
    "ソフトマックス関数には特徴があり、出力の総和が1になる。また、出力それぞれは0から1.0の間の実数になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = [ 0.01821127  0.24519181  0.73659691]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0.3, 2.9, 4.0])\n",
    "y = softmax(a)\n",
    "print(\"y =\", y)\n",
    "np.sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この性質のおかげで、ソフトマックス関数の出力を「確率」として解釈することができる。\n",
    "\n",
    "ソフトマックス関数は単調増加するため、出力の一番大きなニューロンがどれかはソフトマックス関数を適用しなくても分かる。そのため、実際の問題では出力層のソフトマックス関数は省略するのが一般的。（指数関数の計算コストを省略）\n",
    "\n",
    "※省略するのは推論の場合のみ。学習時には省略しない。詳細は次章で取り扱う。\n",
    "\n",
    "### 出力層のニューロンの数\n",
    "\n",
    "多クラス分類問題では、次の図のように、分類したいクラスの数を出力層のニューロン数に設定するのが一般的である。\n",
    "\n",
    "図3-23\n",
    "\n",
    "## 3.6 手書き数字認識\n",
    "\n",
    "ここから実践的な問題に取り組む。学習はすでに完了したものとして、学習済みのモデルを用いて推論処理を実装する。なお、この推論処理は、ニューラルネットワークの順方向伝播 (forward propagation) ともいう。\n",
    "\n",
    "### 3.6.1 MNISTデータセット\n",
    "\n",
    "データセットはMNISTという手書き数字の画像セットを利用する。\n",
    "\n",
    "※ちなみにMNISTはエムニストと読むらしい。\n",
    "\n",
    "データセットの構成は以下\n",
    "\n",
    "- 0から9までの数字画像\n",
    "- 訓練画像が60,000枚\n",
    "- テスト画像が10,000枚\n",
    "- 28x28のグレースケール (1チャンネル)\n",
    "- 各ピクセルは0から255の値を取る\n",
    "- それぞれの画像データに対しては対応するラベルが与えられている (7, 2, 1など)\n",
    "\n",
    "書籍の付録で、MNISTデータセットのダウンロードから画像データのNumPy配列への変換までをサポートする便利なスクリプト mnist.py が提供されている。使い方は以下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n",
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初は時間かかるけど、２回目以降はローカルに保存したファイル (pickleファイル) の読み込みを行うだけなのですぐに処理が終了する。\n",
    "\n",
    "オプションについては、normalize=Trueで0~255の値を0.0~1.0の値に正規化する。 flatten=Trueで入力画像を1次元配列にする。Falseなら1x28x28の3次元配列となる。one_hot_label=Trueのときは結果がone-hot表現で返される。Falseなら単に正解のラベルが返却される。\n",
    "\n",
    "まずデータの確認をする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 5\n",
      "img.shape = (784,)\n",
      "img.shape = (28, 28)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "\n",
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "print(\"label:\", label)\n",
    "\n",
    "print(\"img.shape =\", img.shape)\n",
    "img = img.reshape(28, 28)\n",
    "print(\"img.shape =\", img.shape)\n",
    "\n",
    "img_show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten=Trueで読み込んでいるので、reshapeしてからimg_showに食わせている。\n",
    "\n",
    "NumPy配列からPIL用のデータオブジェクトへの変換はImage.fromarray()で行っている。\n",
    "\n",
    "### 3.6.2 ニューラルネットワークの推論処理\n",
    "\n",
    "推論処理を実装する。\n",
    "\n",
    "ネットワークは、入力層を784個、出力層10個のネットワークで構成する。\n",
    "\n",
    "隠れ層を2つ、1つ目が50個、2つ目が100個のニューロンを持つとする。この50, 100という数字は任意の値に設定できる。\n",
    "\n",
    "まず3つの関数 get_data, init_network, predictを定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import pickle\n",
    "from dataset.mnist import load_mnist\n",
    "from common.functions import sigmoid, softmax\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "def init_network():\n",
    "    with open(\"sample_weight.pkl\", 'rb') as f:\n",
    "        network = pickle.load(f)\n",
    "        \n",
    "    return network\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_weight.pklには、学習済みの重みとバイアスのパラメータが辞書型の変数として保存されている。\n",
    "\n",
    "推論してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "# MNISTデータセットを取得、ネットワークを生成\n",
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "accuracy_cnt = 0\n",
    "for i in range(len(x)):\n",
    "    # 画像を1枚ずつ取り出し、predict関数で分類\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y) # 最も確率の高い要素のインデックスを取得\n",
    "    # 予測結果pを正解ラベルtと照合\n",
    "    if (p == t[i]):\n",
    "        accuracy_cnt += 1\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:0.9352とは、93.52%正しく分類することができたことを表す。次章以降ではこの認識精度をさらに高めていく。\n",
    "\n",
    "### 3.6.3 バッチ処理\n",
    "\n",
    "Courseraで最初からやっていたように、データセットをまとめて行列計算することをバッチ処理という。\n",
    "\n",
    "実装は次のようにできる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch, axis=1)\n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列計算ライブラリの最適化された実装により、バッチ処理した方が計算が速くなることが多い。\n",
    "\n",
    "## 3.7 まとめ\n",
    "\n",
    "本章では、ニューラルネットワークの順伝播 (forward propagation) について解説した。\n",
    "\n",
    "# 感想\n",
    "\n",
    "NumPyを利用することで簡単に行列計算を実装できた。\n",
    "\n",
    "意外とボリュームがあったが、forward propagationについてはすでにCourseraで勉強していたため、それほど躓くことなく進められた。Courseraでも苦労していたbackward propagationが次章で取り扱われるが、ここを頑張って乗り越えて、しっかりと理解したい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
