{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmaxレイヤの実装\n",
    "\n",
    "前のセクションまでで乗算、加算、ReLU, Sigmoidといった計算レイヤの逆伝播を実装してきた。\n",
    "\n",
    "参考： https://sge.qiita.com/yoshinari_yuto/items/e07fc89c36e123a58b49\n",
    "\n",
    "このセクションからニューラルネットワークの逆伝播を実装していく。具体的には、行列の積（幾何学でアフィン変換とよばれる）における逆伝播を考える。\n",
    "\n",
    "### 5.6.1 Affineレイヤ\n",
    "\n",
    "ニューラルネットワークの順伝播は次のように実装した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(2) # 入力\n",
    "W = np.random.rand(2, 3) # 重み\n",
    "B = np.random.rand(3) # バイアス\n",
    "\n",
    "Y = np.dot(X, W) + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらにこのYは活性化関数によって変換され、次の層へ伝播される。\n",
    "\n",
    "これまでのように、ここで行った行列の積とバイアスの和を計算グラフで表してみる。\n",
    "\n",
    "図5-24\n",
    "\n",
    "この図は比較的単純なグラフだが、X, W, Bがスカラ値ではなく行列であることに注意して逆伝播を考えていく。\n",
    "\n",
    "図5-25\n",
    "\n",
    "最後のレイヤは単純に偏微分すれば良い。\n",
    "\n",
    "$$\n",
    "{\\bf Y} = (y_1, y_2, y_3), \\\\\n",
    "\\frac{\\partial L}{\\partial {\\bf Y}} =\n",
    "\\left ( \\frac{\\partial L}{\\partial y_1}, \\frac{\\partial L}{\\partial y_2}, \\frac{\\partial L}{\\partial y_3} \\right )\n",
    "$$\n",
    "\n",
    "次の加算レイヤの逆伝播はそのまま出力すればよかった。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial ({\\bf X} \\cdot {\\bf W})} = \\frac{\\partial L}{\\partial {\\bf Y}}, \\\\\n",
    "\\frac{\\partial L}{\\partial {\\bf B}} = \\frac{\\partial L}{\\partial {\\bf Y}}\n",
    "$$\n",
    "\n",
    "最後のAffineレイヤ (アフィン変換を行う処理) は\n",
    "\n",
    "$$\n",
    "{\\bf F} = {\\bf X} \\cdot {\\bf W}\n",
    "$$\n",
    "と置いて、\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial {\\bf X}} = \\frac{\\partial L}{\\partial {\\bf F}} \\cdot \\frac{\\partial {\\bf F}}{\\partial {\\bf X}} =\n",
    "\\frac{\\partial L}{\\partial ({\\bf X} \\cdot {\\bf W})} \\cdot \\frac{\\partial ({\\bf X} \\cdot {\\bf W})}{\\partial {\\bf X}}\n",
    "= \\frac{\\partial L}{\\partial {\\bf Y}} \\cdot \\frac{\\partial ({\\bf X} \\cdot {\\bf W})}{\\partial {\\bf X}}\n",
    "$$\n",
    "行列の積ができるように次元を調整して、\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial {\\bf X}} = \\frac{\\partial L}{\\partial {\\bf Y}} \\cdot {\\bf W}^{\\mathrm{T}}\n",
    "$$\n",
    "\n",
    "図5-26\n",
    "\n",
    "Wについても同様に\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial {\\bf W}} = {\\bf X}^{\\mathrm{T}} \\cdot \\frac{\\partial L}{\\partial {\\bf Y}}\n",
    "$$\n",
    "\n",
    "\n",
    "### 5.6.2 バッチ版Affineレイヤ\n",
    "\n",
    "前節で扱ったXはベクトルで、一つのデータを対象としていた。\n",
    "次にN個のデータをまとめて順伝播する場合（バッチ）のAffineレイヤを考える。\n",
    "\n",
    "図5-27\n",
    "\n",
    "ここで、バイアス項Bの加算において行列の次元が異なるが、各データに対してWを加算することに注意する。\n",
    "（N個のデータに対してAffine変換を実行すると考えれば自明）\n",
    "\n",
    "計算例としては"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [10, 10, 10]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W = np.array([[0,0,0], [10,10,10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "X_dot_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにバイアスBの加算は各データに対して行われる。そのため、逆伝播の際には各データの逆伝播の値がバイアスに集約される。（そもそも次元も違うため、そうしなければ計算も合わない）\n",
    "\n",
    "プログラムで表すと以下のようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1,2,3], [4,5,6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY, axis=0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上から、Affineレイヤの実装は以下のようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T) # .Tは転置\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Lossレイヤ\n",
    "\n",
    "最後に出力層のSoftmaxレイヤを実装していく。ソフトマックス関数については既に取り扱ったことがあるので、ここでの説明は省略する。\n",
    "\n",
    "損失関数である交差エントロピー誤差 (cross entropy error) も含めて、Softmax-with-Lossレイヤという名前で実装する。\n",
    "計算グラフは以下の図のようになる\n",
    "\n",
    "図5-29\n",
    "\n",
    "簡易的に表すと次の図のようになる\n",
    "\n",
    "図5-30\n",
    "\n",
    "ここでtはone-hotな教師ラベル（正解データ）である。まずCross entropy errorレイヤの逆伝播を考える。対数関数の微分は\n",
    "\n",
    "$$\n",
    "(\\log (x) )' = \\frac{1}{x}\n",
    "$$\n",
    "\n",
    "だったので、逆伝播を一つ一つ記入していくと下図のようになる\n",
    "\n",
    "図A-4\n",
    "\n",
    "よってCross entropy errorレイヤの逆伝播は\n",
    "\n",
    "$$\n",
    "\\left ( -\\frac{t_1}{y_1}, -\\frac{t_2}{y_2}, -\\frac{t_3}{y_3} \\right )\n",
    "$$\n",
    "\n",
    "となる。続いてSoftmaxレイヤの逆伝播を考える。\n",
    "\n",
    "ステップ1\n",
    "\n",
    "まずCross entropy errorレイヤからの逆伝播の値が流れてくる。\n",
    "\n",
    "ステップ2\n",
    "\n",
    "最初の乗算ノードの片側（除算ノードに向かう方）は次のようになる\n",
    "\n",
    "$$\n",
    "-\\frac{t_1}{y_1} \\exp (a_1) = -\\frac{t_1}{\\frac{\\exp (a_1)}{S}} \\exp (a_1) = -t_1 S\n",
    "$$\n",
    "\n",
    "ステップ3\n",
    "\n",
    "続けて除算の微分をする。この除算ノードは枝分かれして複数のノードに順伝播しており、バイアス項のときと同じように逆伝播では集約（各データを全て加算）される。\n",
    "よって\n",
    "\n",
    "$$\n",
    "\\left ( \\frac{1}{f} \\right )' = -\\frac{f'}{f^2}\n",
    "$$\n",
    "\n",
    "なので、\n",
    "\n",
    "$$\n",
    "-S (t_1 + t_2 + t_3) \\cdot \\left ( \\frac{1}{S} \\right )' = S (t_1 + t_2 + t_3) \\cdot \\frac{1}{S^2} = \\frac{1}{S} (t_1 + t_2 + t_3)\n",
    "$$\n",
    "\n",
    "ステップ4\n",
    "\n",
    "またtはone-hotラベルであったので、その総和は1となり、このノードの逆伝播は\n",
    "\n",
    "$$\n",
    "\\frac{1}{S}\n",
    "$$\n",
    "\n",
    "となる。加算ノードはそのまま流すだけ。\n",
    "\n",
    "ステップ5\n",
    "\n",
    "加算ノードのもう片方（expノードに向かう方）は\n",
    "\n",
    "$$\n",
    "-\\frac{t_1}{y_1} \\cdot \\frac{1}{S} = -\\frac{t_1}{\\frac{\\exp (a_1)}{S}} \\cdot \\frac{1}{S} = - \\frac{t_1}{\\exp(a_1)}\n",
    "$$\n",
    "\n",
    "となる。\n",
    "\n",
    "ステップ6\n",
    "\n",
    "最後にexpノードだが、expは微分しても変わらないので、\n",
    "\n",
    "$$\n",
    "\\left ( \\frac{1}{S} - \\frac{t_1}{\\exp(a_1)} \\right ) \\cdot (\\exp (a_1))' = \\left ( \\frac{1}{S} - \\frac{t_1}{\\exp(a_1)} \\right ) \\cdot \\exp (a_1)\n",
    "= \\frac{\\exp (a_1)}{S} - t_1 = y_1 - t_1\n",
    "$$\n",
    "\n",
    "となる。以上から、順伝播の入力がa1のノードでは、逆伝播がy1-t1となることが導かれた。\n",
    "逆伝播から最終的に得るベクトルは\n",
    "\n",
    "$$\n",
    "(y_1 - t_1, \\, y_2 - t_2, \\, y_3 - t_3)\n",
    "$$\n",
    "\n",
    "となる。これはたまたまきれいな結果が出たわけではなく、逆伝播が計算しやすくなるように交差エントロピー誤差が定義されている。\n",
    "\n",
    "ここで具体例として、教師ラベルが (0,1,0) のようなデータに対して、Softmaxレイヤの出力が (0.3, 0.2, 0.5) 出会った場合を考える。\n",
    "正解ラベルに対する確率は0.2なので、この時点では学習がまだまだ進んでいないことがわかる。\n",
    "実際にこのレイヤの逆伝播は (0.3, -0.8, 0.5) となり、大きな誤差を伝播することになる。\n",
    "\n",
    "重みを変えて学習を続け、教師ラベルが (0,1,0) のようなデータに対してSoftmaxレイヤの出力が (0.01, 0.99, 0) となった場合を考える。\n",
    "この場合Softmaxレイヤからの逆伝播は (0.01, -0.01, 0) という小さな値になる。\n",
    "\n",
    "Softmax-with-Lossレイヤの実装は以下のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # 順伝播の出力\n",
    "        self.t = None # 教師データ (one-hot vector)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで逆伝播の結果をバッチサイズで割っているが、データ1個あたりの誤差を伝播させるためにこうしている。\n",
    "\n",
    "## 5.7 誤差逆伝播法の実装\n",
    "\n",
    "これまで実装したレイヤを組み合わせることでニューラルネットワークの構築ができる。\n",
    "\n",
    "### 5.7.1 ニューラルネットワークの学習の全体図\n",
    "\n",
    "**前提**\n",
    "```\n",
    "ニューラルネットワークは、適応可能な重みとバイアスがあり、\n",
    "この重みとバイアスを訓練データに適応するように調整することを「学習」とよぶ。\n",
    "ニューラルネットワークの学習は次の4つの手順で行う。\n",
    "```\n",
    "\n",
    "**ステップ1（ミニバッチ）**\n",
    "```\n",
    "訓練データの中からランダムに一分のデータを取り出す。\n",
    "```\n",
    "\n",
    "**ステップ2（勾配の算出）**\n",
    "```\n",
    "各重みパラメータに関する損失関数の勾配を求める。\n",
    "```\n",
    "\n",
    "**ステップ3（パラメータの更新）**\n",
    "```\n",
    "重みパラメータを勾配方向に微小量だけ更新する。\n",
    "```\n",
    "\n",
    "**ステップ4（繰り返す）**\n",
    "```\n",
    "ステップ1~3を繰り返す。\n",
    "```\n",
    "\n",
    "第4章ではこの勾配を求めるために数値微分を利用したが、誤差逆伝播法を用いれば効率よく勾配を求めることができる。\n",
    "\n",
    "### 5.7.2 誤差逆伝播法に対応したニューラルネットワークの実装\n",
    "\n",
    "基本的には4章の実装と同じだが、ここではレイヤを保持する順序付きディクショナリ (OrderedDict) を定義している。\n",
    "レイヤを使用することで、predictやgradientをレイヤの伝播だけで実装できるようになる。\n",
    "\n",
    "2層のニューラルネットワーク TwoLayerNet を実装していくが、このクラスのインスタンス変数とメソッドを下表に整理しておく。\n",
    "\n",
    "表5-1\n",
    "\n",
    "表5-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x) # レイヤの伝播\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1) # one-hotラベルを通常のラベルに戻す\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse() # 逆順でレイヤを伝播\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにニューラルネットワークの構成要素をレイヤとして実装したことで、ニューラルネットワークを簡単に実装することが出来た。\n",
    "\n",
    "### 5.7.3 誤差逆伝播法の勾配確認\n",
    "\n",
    "これまで勾配を求める方法を2つ取り扱ってきた。\n",
    "\n",
    "- 数値微分によって求める方法\n",
    "- 解析的に数式を解いて求める方法 (誤差逆伝播法)\n",
    "\n",
    "数値微分による方法は計算に時間がかかるため学習で使われることはないが、実装が簡単なため、より複雑な誤差逆伝播法の実装をデバッグするときに利用できる。\n",
    "\n",
    "数値微分の結果と誤差逆伝播法の結果を比較して、十分近い値が得られるか確認する作業を **勾配確認** (gradient check) という。\n",
    "\n",
    "勾配確認の実装は次のようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:4.16578047279e-10\n",
      "b1:2.45278031513e-09\n",
      "W2:5.30048849398e-09\n",
      "b2:1.39954044853e-07\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のような実行結果が得られ、誤差が十分に小さく、よって誤差逆伝播法が正しく実装できていると判断できる。\n",
    "\n",
    "### 5.7.4 誤差逆伝播法を使った学習\n",
    "\n",
    "学習については4章でやった方法と同じなので省略する。\n",
    "\n",
    "4章では数値微分によって勾配を求めたが、今回実装した誤差逆伝播法によって勾配を求めることに注意する。\n",
    "\n",
    "\n",
    "## 5.8 まとめ\n",
    "\n",
    "- 計算グラフを用いれば、計算過程を視覚的に把握することが出来る\n",
    "- 計算グラフのノードは局所的な計算によって構成される。局所的な計算が全体の計算を構成する\n",
    "- 計算グラフの順伝播は、通常の計算を行う。一方、計算グラフの逆伝播によって、各ノードの微分を求めることが出来る\n",
    "- ニューラルネットワークの構成要素をレイヤとして実装することで、勾配の計算を効率的に求めることが出来る（誤差逆伝播法）\n",
    "- 数値微分と誤差逆伝播法の結果を比較することで、誤差逆伝播法の実装に誤りがないことを確認できる（勾配確認）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
