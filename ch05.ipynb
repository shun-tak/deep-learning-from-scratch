{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmaxレイヤの実装\n",
    "\n",
    "前のセクションまでで乗算、加算、ReLU, Sigmoidといった計算レイヤの逆伝播を実装してきた。\n",
    "\n",
    "参考： https://sge.qiita.com/yoshinari_yuto/items/e07fc89c36e123a58b49\n",
    "\n",
    "このセクションからニューラルネットワークの逆伝播を実装していく。具体的には、行列の積（幾何学でアフィン変換とよばれる）における逆伝播を考える。\n",
    "\n",
    "### 5.6.1 Affineレイヤ\n",
    "\n",
    "ニューラルネットワークの順伝播は次のように実装した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(2) # 入力\n",
    "W = np.random.rand(2, 3) # 重み\n",
    "B = np.random.rand(3) # バイアス\n",
    "\n",
    "Y = np.dot(X, W) + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらにこのYは活性化関数によって変換され、次の層へ伝播される。\n",
    "\n",
    "これまでのように、ここで行った行列の積とバイアスの和を計算グラフで表してみる。\n",
    "\n",
    "図5-24\n",
    "\n",
    "この図は比較的単純なグラフだが、X, W, Bがスカラ値ではなく行列であることに注意して逆伝播を考えていく。\n",
    "\n",
    "図5-25\n",
    "\n",
    "最後のレイヤは単純に偏微分すれば良い。\n",
    "\n",
    "$$\n",
    "{\\bf Y} = (y_1, y_2, y_3), \\\\\n",
    "\\frac{\\partial L}{\\partial {\\bf Y}} =\n",
    "\\left ( \\frac{\\partial L}{\\partial y_1}, \\frac{\\partial L}{\\partial y_2}, \\frac{\\partial L}{\\partial y_3} \\right )\n",
    "$$\n",
    "\n",
    "次の加算レイヤの逆伝播はそのまま出力すればよかった。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial ({\\bf X} \\cdot {\\bf W})} = \\frac{\\partial L}{\\partial {\\bf Y}}, \\\\\n",
    "\\frac{\\partial L}{\\partial {\\bf B}} = \\frac{\\partial L}{\\partial {\\bf Y}}\n",
    "$$\n",
    "\n",
    "最後のAffineレイヤ (アフィン変換を行う処理) は\n",
    "\n",
    "$$\n",
    "{\\bf F} = {\\bf X} \\cdot {\\bf W}\n",
    "$$\n",
    "と置いて、\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial {\\bf X}} = \\frac{\\partial L}{\\partial {\\bf F}} \\cdot \\frac{\\partial {\\bf F}}{\\partial {\\bf X}} =\n",
    "\\frac{\\partial L}{\\partial ({\\bf X} \\cdot {\\bf W})} \\cdot \\frac{\\partial ({\\bf X} \\cdot {\\bf W})}{\\partial {\\bf X}}\n",
    "= \\frac{\\partial L}{\\partial {\\bf Y}} \\cdot \\frac{\\partial ({\\bf X} \\cdot {\\bf W})}{\\partial {\\bf X}}\n",
    "$$\n",
    "行列の積ができるように次元を調整して、\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial {\\bf X}} = \\frac{\\partial L}{\\partial {\\bf Y}} \\cdot {\\bf W}^{\\mathrm{T}}\n",
    "$$\n",
    "\n",
    "図5-26\n",
    "\n",
    "Wについても同様に\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial {\\bf W}} = {\\bf X}^{\\mathrm{T}} \\cdot \\frac{\\partial L}{\\partial {\\bf Y}}\n",
    "$$\n",
    "\n",
    "\n",
    "### 5.6.2 バッチ版Affineレイヤ\n",
    "\n",
    "前節で扱ったXはベクトルで、一つのデータを対象としていた。\n",
    "次にN個のデータをまとめて順伝播する場合（バッチ）のAffineレイヤを考える。\n",
    "\n",
    "図5-27\n",
    "\n",
    "ここで、バイアス項Bの加算において行列の次元が異なるが、各データに対してWを加算することに注意する。\n",
    "（N個のデータに対してAffine変換を実行すると考えれば自明）\n",
    "\n",
    "計算例としては"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [10, 10, 10]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W = np.array([[0,0,0], [10,10,10]])\n",
    "B = np.array([1,2,3])\n",
    "\n",
    "X_dot_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにバイアスBの加算は各データに対して行われる。そのため、逆伝播の際には各データの逆伝播の値がバイアスに集約される。（そもそも次元も違うため、そうしなければ計算も合わない）\n",
    "\n",
    "プログラムで表すと以下のようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1,2,3], [4,5,6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY, axis=0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上から、Affineレイヤの実装は以下のようになる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b): # 慣習としてベクトルは小文字で表される\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T) # .Tは転置\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Lossレイヤ\n",
    "\n",
    "最後に出力層のSoftmaxレイヤを実装していく。ソフトマックス関数については既に取り扱ったことがあるので、ここでの説明は省略する。\n",
    "\n",
    "損失関数である交差エントロピー誤差 (cross entropy error) も含めて、Softmax-with-Lossレイヤという名前で実装する。\n",
    "計算グラフは以下の図のようになる\n",
    "\n",
    "図5-29\n",
    "\n",
    "簡易的に表すと次の図のようになる\n",
    "\n",
    "図5-30\n",
    "\n",
    "ここでtはone-hotな教師ラベル（正解データ）である。まずCross entropy errorレイヤの逆伝播を考える。対数関数の微分は\n",
    "\n",
    "$$\n",
    "(\\log (x) )' = \\frac{1}{x}\n",
    "$$\n",
    "\n",
    "だったので、逆伝播を一つ一つ記入していくと下図のようになる\n",
    "\n",
    "図A-4\n",
    "\n",
    "よってCross entropy errorレイヤの逆伝播は\n",
    "\n",
    "$$\n",
    "\\left ( -\\frac{t_1}{y_1}, -\\frac{t_2}{y_2}, -\\frac{t_3}{y_3} \\right )\n",
    "$$\n",
    "\n",
    "となる。続いてSoftmaxレイヤの逆伝播を考える。\n",
    "\n",
    "ステップ1\n",
    "\n",
    "まずCross entropy errorレイヤからの逆伝播の値が流れてくる。\n",
    "\n",
    "ステップ2\n",
    "\n",
    "最初の乗算ノードの片側（除算ノードに向かう方）は次のようになる\n",
    "\n",
    "$$\n",
    "-\\frac{t_1}{y_1} \\exp (a_1) = -\\frac{t_1}{\\frac{\\exp (a_1)}{S}} \\exp (a_1) = -t_1 S\n",
    "$$\n",
    "\n",
    "ステップ3\n",
    "\n",
    "続けて除算の微分をする。この除算ノードは枝分かれして複数のノードに順伝播しており、バイアス項のときと同じように逆伝播では集約（各データを全て加算）される。\n",
    "よって\n",
    "\n",
    "$$\n",
    "\\left ( \\frac{1}{f} \\right )' = -\\frac{f'}{f^2}\n",
    "$$\n",
    "\n",
    "なので、\n",
    "\n",
    "$$\n",
    "-S (t_1 + t_2 + t_3) \\cdot \\left ( \\frac{1}{S} \\right )' = S (t_1 + t_2 + t_3) \\cdot \\frac{1}{S^2} = \\frac{1}{S} (t_1 + t_2 + t_3)\n",
    "$$\n",
    "\n",
    "ステップ4\n",
    "\n",
    "またtはone-hotラベルであったので、その総和は1となり、このノードの逆伝播は\n",
    "\n",
    "$$\n",
    "\\frac{1}{S}\n",
    "$$\n",
    "\n",
    "となる。加算ノードはそのまま流すだけ。\n",
    "\n",
    "ステップ5\n",
    "\n",
    "加算ノードのもう片方（expノードに向かう方）は\n",
    "\n",
    "$$\n",
    "-\\frac{t_1}{y_1} \\cdot \\frac{1}{S} = -\\frac{t_1}{\\frac{\\exp (a_1)}{S}} \\cdot \\frac{1}{S} = - \\frac{t_1}{\\exp(a_1)}\n",
    "$$\n",
    "\n",
    "となる。\n",
    "\n",
    "ステップ6\n",
    "\n",
    "最後にexpノードだが、expは微分しても変わらないので、\n",
    "\n",
    "$$\n",
    "\\left ( \\frac{1}{S} - \\frac{t_1}{\\exp(a_1)} \\right ) \\cdot (\\exp (a_1))' = \\left ( \\frac{1}{S} - \\frac{t_1}{\\exp(a_1)} \\right ) \\cdot \\exp (a_1)\n",
    "= \\frac{\\exp (a_1)}{S} - t_1 = y_1 - t_1\n",
    "$$\n",
    "\n",
    "となる。以上から、順伝播の入力がa1のノードでは、逆伝播がy1-t1となることが導かれた。\n",
    "逆伝播から最終的に得るベクトルは\n",
    "\n",
    "$$\n",
    "(y_1 - t_1, \\, y_2 - t_2, \\, y_3 - t_3)\n",
    "$$\n",
    "\n",
    "となる。これはたまたまきれいな結果が出たわけではなく、逆伝播が計算しやすくなるように交差エントロピー誤差が定義されている。\n",
    "\n",
    "ここで具体例として、教師ラベルが (0,1,0) のようなデータに対して、Softmaxレイヤの出力が (0.3, 0.2, 0.5) 出会った場合を考える。\n",
    "正解ラベルに対する確率は0.2なので、この時点では学習がまだまだ進んでいないことがわかる。\n",
    "実際にこのレイヤの逆伝播は (0.3, -0.8, 0.5) となり、大きな誤差を伝播することになる。\n",
    "\n",
    "重みを変えて学習を続け、教師ラベルが (0,1,0) のようなデータに対してSoftmaxレイヤの出力が (0.01, 0.99, 0) となった場合を考える。\n",
    "この場合Softmaxレイヤからの逆伝播は (0.01, -0.01, 0) という小さな値になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
